{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YepYU3SdSa1-"
   },
   "source": [
    "# Exercise 3 - Putting it all together - Model Training\n",
    "\n",
    "In this notebook we will tie everything together to and use what we have learnt to try our hand in the competition.\n",
    "\n",
    "The steps are similar to Exercise 2:\n",
    "1. Explore the competition dataset\n",
    "2. Build a convnet from scratch that performs reasonably well\n",
    "3. Evaluate training and validation accuracy\n",
    "4. Score the model against test set and submit result\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "62EE4dk9Zw-M"
   },
   "source": [
    "## Download and Explore the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qjrw2VMDUJC6"
   },
   "source": [
    "Let's start by downloading our dataset, a .zip of 1,226 PNG pictures of different poses, and extracting it locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YvMUL9IKVuP-"
   },
   "source": [
    "The contents of the `.zip` are extracted to the base directory, which contains `train` and `val` subdirectories for you to do training and validation. The folders have the following structure:\n",
    "\n",
    "```\n",
    "---------------\n",
    "train\n",
    "|- ChairPose\n",
    "|- ChildPose\n",
    "|- Dabbing\n",
    "|- HandGun\n",
    "|- HandShake\n",
    "|- HulkSmash\n",
    "|- KoreanHeart\n",
    "|- KungfuCrane\n",
    "|- KungfuSalute\n",
    "|- Salute\n",
    "|- WarriorPose\n",
    "\n",
    "val\n",
    "|- ChairPose\n",
    "|- ChildPose\n",
    "|- Dabbing\n",
    "|- HandGun\n",
    "|- HandShake\n",
    "|- HulkSmash\n",
    "|- KoreanHeart\n",
    "|- KungfuCrane\n",
    "|- KungfuSalute\n",
    "|- Salute\n",
    "|- WarriorPose\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kzEr2gZ05RHX"
   },
   "outputs": [],
   "source": [
    "# Creating two directories - \"data\" and \"data/trainset_11classes_0_00000\" \n",
    "!mkdir data && mkdir data/trainset_11classes_0_00000\n",
    "# Downloading the ai-camp competition dataset\n",
    "!wget -N https://ai-camp.s3-us-west-2.amazonaws.com/trainset_11classes_0_00000.zip\n",
    "# Unzip the data into the folder \"data/trainset_11classes_0_00000\"\n",
    "!unzip -qq -n trainset_11classes_0_00000.zip -d data/trainset_11classes_0_00000\n",
    "# Switch directory to \"data/trainset_11classes_0_00000\" and show its content\n",
    "!cd data/trainset_11classes_0_00000 && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4YPMa22yaJA-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = 'data/trainset_11classes_0_00000'\n",
    "\n",
    "# Directory to our training data\n",
    "train_folder = os.path.join(base_dir, 'train')\n",
    "\n",
    "# Directory to our validation data\n",
    "val_folder = os.path.join(base_dir, 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bVCvH_XXuTlu"
   },
   "source": [
    "Now, let's find out the total number of images we have in each `train`, `val` and `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SIWrHBu1awaC"
   },
   "outputs": [],
   "source": [
    "# List folders and number of files\n",
    "print(\"Directory, Number of files\")\n",
    "for root, subdirs, files in os.walk(base_dir):\n",
    "    print(root, len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LvtQnO4CwLtS"
   },
   "source": [
    "We can see that there are 11 categories/folders in each `train` and `val` folder.\n",
    "\n",
    "Now let's take a look at a few images to get a better sense of what the `KoreanHeart` and `KungfuCrane` categories look like. First, configure the matplotlib parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z8jsWloowYpL"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Parameters for our graph; we'll output images in a 4x4 configuration\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "\n",
    "# Index for iterating over images\n",
    "pic_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xPDMLSoUxksu"
   },
   "source": [
    "Now, display a batch of 8 KoreanHeart and 8 KungfuCrane poses. You can rerun the cell to see a new batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uxtrAyx9wa5i"
   },
   "outputs": [],
   "source": [
    "## Path to KoreanHeart and KungfuCrane\n",
    "train_koreanheart_dir= \"data/trainset_11classes_0_00000/train/KoreanHeart\"\n",
    "train_kungfucrane_dir= \"data/trainset_11classes_0_00000/train/KungfuCrane\"\n",
    "train_koreanheart_fnames = os.listdir(train_koreanheart_dir)\n",
    "train_kungfucrane_fnames = os.listdir(train_kungfucrane_dir)\n",
    "\n",
    "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 15)\n",
    "\n",
    "pic_index += 8\n",
    "next_koreanheart_pix = [os.path.join(train_koreanheart_dir, fname) \n",
    "                for fname in train_koreanheart_fnames[pic_index-8:pic_index]]\n",
    "next_kungfucrane_pix = [os.path.join(train_kungfucrane_dir, fname) \n",
    "                for fname in train_kungfucrane_fnames[pic_index-8:pic_index]]\n",
    "\n",
    "for i, img_path in enumerate(next_koreanheart_pix+next_kungfucrane_pix):\n",
    "    # Set up subplot; subplot indices start at 1\n",
    "    sp = plt.subplot(nrows, ncols, i + 1)\n",
    "    sp.axis('Off') # Don't show axes (or gridlines)\n",
    "    \n",
    "    img = mpimg.imread(img_path)\n",
    "    plt.imshow(img)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_boVgbVAUcVF"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Let's set up data generators that will read images from our source folders and convert them to float32 tensors. We'll have one generator for each training and validation folders.\n",
    "\n",
    "### Batch\n",
    "Our generators will yield batches of `32` images of size `299 x 299` and their labels.\n",
    "\n",
    "### Feature scaling\n",
    "Recall that in our MNIST/CIFAR-10 exercises, data that goes into a neural network should be normalised in a way that is easier to be processed by the network. In our case, we will preprocess our images by normalising the pixels values to be in the 0 to 1 range. This happens by dividing each pixel value by 255 and this process is known as data normalisation or rescaling.\n",
    "\n",
    "### Generator - ImageDataGenerator\n",
    "To rescale the data, we use `keras.preprocessing.image.ImageDataGenerator` class with the `rescale` parameter. This class will also allow us to instantiate generators of augmented image batches (and their labels) via `.flow_from_directory(directory)`. These generators can then be used with the Keras model methods that accept data generators as inputs such as `fit_generator`, `evaluate_generator` and `predict_generator`. We used data augmentation for the training image generator. To find out more about how to do image augmentation in keras, go [here](https://keras.io/preprocessing/image/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kXpGgyCWSF0u"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Batch size\n",
    "bs = 32\n",
    "\n",
    "# All images will be resized to this value\n",
    "image_size = (299, 299)\n",
    "\n",
    "# All images will be rescaled by 1./255. We apply data augmentation here.\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   brightness_range= [0.5,1.5],\n",
    "                                   horizontal_flip=True)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Flow training images in batches of 32 using train_datagen generator\n",
    "print(\"Preparing generator for train dataset\")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory= train_folder, # This is the source directory for training images \n",
    "    target_size=image_size, # All images will be resized to value set in image_size\n",
    "    batch_size=bs,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Flow validation images in batches of 32 using val_datagen generator\n",
    "print(\"Preparing generator for validation dataset\")\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    directory= val_folder, \n",
    "    target_size=image_size,\n",
    "    batch_size=bs,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "suFcRHxkZIVr"
   },
   "source": [
    "## Building a Small Convnet Model\n",
    "\n",
    "The images that will go into our convnet are **299 x 299** color images. You are free to resize the images for faster training time or better accuracy.\n",
    "\n",
    "Here, we designed a very simple architecture. It consists of three convolutional layers of stride 2 ending with a [global average pooling layer](https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/). In this architecture we decided not to use a series of dense layers but instead went straight to the final softmax classification layer. Feel free to modify the model to improve the accuracy. We have introduced here a few new techniques different from the preceeding notebooks but there are many more out there that you can explore and use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFYlZE4DTWtO"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, BatchNormalization, Activation, GlobalAveragePooling2D, Dense, Dropout, MaxPooling2D\n",
    "\n",
    "# Here we specify the input shape of our data \n",
    "# This should match the size of images ('image_size') along with the number of channels (3)\n",
    "input_shape = (299, 299, 3)\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = 11\n",
    "\n",
    "# Defining a baseline model. Here we use the [keras functional api](https://keras.io/getting-started/functional-api-guide) to build the model. \n",
    "# TODO: explore different architectures and training schemes\n",
    "input_img = Input(shape=input_shape)\n",
    "x = Conv2D(16, (3, 3), padding='same', strides=2)(input_img)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x = Conv2D(16, (3, 3), padding='same', strides=2)(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x = Conv2D(16, (3, 3), padding='same', strides=2)(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x = Conv2D(16, (3, 3), padding='same', strides=2)(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_img, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Av-Mk7KZKD7Q"
   },
   "source": [
    "Let's summarise the model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eEwSI4-FKDg4"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5jFT6kbPZ_Wi"
   },
   "source": [
    "Next, we will configure the specifications for model training.\n",
    "\n",
    "We train our model with `categorical_crossentropy` loss, because this is a multi-class problem. We will use the `adam` optimizer with default settings. During training, we want to monitor `accuracy` of the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Xmx5IvQZ4oK"
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WBuT_XpNo2ep"
   },
   "source": [
    "## Setting Up Checkpoints\n",
    "\n",
    "Let's setup a [checkpoint](https://keras.io/callbacks/) to help us monitor the validation accuracy as the model trains. This checkpoint will save the model with best validation accuracy seen so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cBPWDUcGo3P4"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "bestValidationCheckpointer = ModelCheckpoint('saved_model.hdf5', monitor='val_acc', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T0rPDjqYaEmq"
   },
   "source": [
    "## Model Training \n",
    "\n",
    "Let's train on all the images in the training set, for 100 epochs, and validate against all validation images.\n",
    "\n",
    "Note: This may take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 13688
    },
    "colab_type": "code",
    "id": "bzU1gNrcTPX3",
    "outputId": "ca467c8d-d643-4feb-868b-27d920816e99"
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "        train_generator, # train generator has 973 train images\n",
    "        steps_per_epoch=train_generator.samples // bs + 1,\n",
    "        epochs=200,\n",
    "        validation_data=val_generator, # validation generator has 253 validation images\n",
    "        validation_steps=val_generator.samples // bs + 1,\n",
    "        callbacks=[bestValidationCheckpointer]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kFVkjJnBfCe_"
   },
   "source": [
    "## Evaluating Accuracy and Loss of the Model\n",
    "\n",
    "With a trained model, we can evaluate the model performance against the truth labels of our validation set. First, we load the best model encountered during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ywpKyHBZSF02"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model_path = 'saved_model.hdf5'\n",
    "model = load_model( model_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mkv8rOMYs0mj"
   },
   "source": [
    "Then, we validate accuracy of the loaded model on our good old validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1jEaPXg5s047"
   },
   "outputs": [],
   "source": [
    "val_generator.reset()\n",
    "\n",
    "scores = model.evaluate_generator(val_generator, steps=val_generator.samples // val_generator.batch_size + 1, verbose=1)\n",
    "print('Val loss:', scores[0])\n",
    "print('Val accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "3_Image_Classification_TILCompetition.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
